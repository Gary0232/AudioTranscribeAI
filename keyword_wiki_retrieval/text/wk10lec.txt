SPEAKER 0
basically, uh, this is where we stopped last time. Um, so we'll start today by, um, discussing the challenges of sketch based image retrieval. Um, and so here, um, we introduced two critical challenges here. Um, the first one, is shown in the slide. So it's about how to bridge the gap between a natural image shown on the left and then a query sketch shown on the on the right. Um, so the first challenge basically just lies in the inherent difference. Um, between, um the detailed photograph and then a user sketch. So, um, in order to achieve very good image retrieval performance, an S PR IR system, um, needs to bridge this gap by understanding the core visual elements within a sketch and then finding images that share those elements. And, um, to make the the more challenging part is that, um, you know, in many cases, um, the the sketch are of different styles. And then, um, not everyone is good at drawing, so we have to deal with, uh, sketches in a wide variety of styles and conditions. And in most cases, the sketches are not perfect. Perfectly, um, depicted. And, um, this is just to show you, Um, the objective is basically to extract the key, um, edges or the key, Um, visual elements from the, um from the, uh, real world photo so that we know we need to match. Um, the the key edges with the sketch. So, um, that was about the first challenge. And the second challenge, um, lies in dealing with massive image databases. So, um, as shown here, if you're searching through millions or even billions of images, um, the system needs an efficient indexing structure. And if you have, you have to have an efficient indexing structure. Um, to quickly find the potential matches of your sketch, Um, and to ensure you that you get the results in real time without having to wait for, um, hours or days. So, um, to overcome these challenges is important, um, to have good sketch representations. All right, so now our objective has become to, um, find a very good representation for, um sketches. So we need to be able to represent and describe the sketch properly in order to achieve good, um, retrieval performance. However, when we think about, um representing sketch, um, there are also several challenges here. So the first one is, um, how to effectively capture the essence of a sketch. So extracting the most representative curves and edges is very crucial. So if you think about the problem, um, a sketch often focuses on capturing the general shape and location of objects and not not necessarily all the exact details. And an SBIR system needs to be able to understand this distinction and prioritise the shape, content, and then the shape position. So basically, we expect the, uh, the S PR system to be able to appreciate the key visual elements. Um, from a real world photo in order to find, uh, good matches with, um sketches. And so the second challenge here, um, it's about balancing the, um matching efficiency and the effectiveness. So we want the system to find relevant relevant images very quickly and but also want to ensure that those results are truly accurate. And then finally, um, we have the last, um, challenge here. So as image databases grow larger, we need a scalable indexing mechanism. So this is like creating a detailed filing system for millions or even billions of images. Um, so we have have to have such a scalable indexing method, Um, so that it can allow the S PR system to locate potential matches for your sketch very quickly. Um, so today we're going to go through several solutions, um, to tackle the above challenges. Um, so here is just a very, um, general overview of it. Um, so the first involves using, um, feature lines. So you may think of these feature lines like fingerprints for your sketch. So here we introduce, um, techniques like the edge segment based features, um, edge histograms. And then we have tensor descriptors. Um, they can all help the system to understand the key shapes and the key lines within your sketch. And then, um and then we're going to talk about a technique called El, So Edo is a sh short form of edge pixel. So, um, this, uh, acts like a specialised filing system for edges, and it allows the S PR system to efficiently locate sketches with similar edge patterns. So, um, let's first look into edge segment based, um S PR Systems. So the key idea here is to, uh, basically, uh, break your sketch down into smaller segments of edges and because we want. We want to match the real world images with the sketches. So we also have to do the same to the real world coloured images as well. So basically, the idea is to break the real world image into smaller segments of edges. And then, uh, it will then analyse, um, analyse these segments and then especially analysing their, um, uh, direction, the length and how these, uh, edge segments are connected. And then once we do the complete the process for all the coloured images, we can kind of, uh, compare the edge segments to compare the different images. And in this way, the system can identify pictures that share similar shapes and similar outlines. So you may, uh, notice that with such a method, we will be able to, um, mainly, uh, focus on the key edges and the key, uh, visual outlines, Uh, and we can ignore the details. Um, that are not that important. So, um, here we'll begin with the step by step procedure of how to extract edge segments from an image. So here, um, uh, this is a process that consists of four key steps. So the first one is edge detection And then this is followed by half transform, followed by line segment extraction and followed by the final step curve segment extraction. And then in the end, we'll get, um, an image like this. So this only contains the salient, um, edge segments from the original image. So now we just, uh, quickly go through each of the steps. Um, so the first, the system, um, performs edge detection. Um, so this is to identify the edges in the image. So here on algorithms like, uh, Sobell or canny philtres are applied to the coloured images. Uh, sorry to the, um, uh, great scale version of the original image. Uh, and this is to highlight areas of significant intensity changes so that it can mark the potential edges within the image. So we have the original image here, and then, um, here we show the output image after we apply. Um, some edge detector, um, and then following this step, once we have the edge maps, um, we have the next step, which is the half transform. Um, so half transform is a powerful technique primarily used for detecting simple geometric shapes like lines, circles, and sometimes even more complex shapes like, um, ellipses in an image. So with this step, we can robustly detect lines and circles in an image. So, for example, if we have, like, an an edge map like this one, and then after we apply the half transform, uh, we are able to detect, um, the key lines in the original image. So in this example, we can see we are able to highlight basically the key, um, boundary of the hand and the key, um, edges on the phone. So, um, once the lines are detected, um, the next step is line segment extraction. Um, so this involves breaking down the detected lines into shorter segments based on certain CRI criteria. Um, such as angle or length threshold. Um, so, um, the math behind this can be quite complex, but here, you just need to know the big idea. So basically, um, the half transformed identifies pixels that lie on a straight lines, but these pixels might not be connected, and they could also belong to different line segments on the same line. So we have this line segment extraction step. Um, so this step, we have two main goals. The first one is to check the continuity of pixels to group them into subsets that belong to individual line segments. So want to group the disconnected, um, pixels. And then the second objective of this step is to, uh, remove a very short line segments to reduce unnecessary edges and noise in the edge map. And then, um, basically, after identifying straight line segments, um, the process just move on to the final step, which is, um uh, which is the curve segment extraction. Um, so this, um, step utilises techniques like curve fitting or supply interpolation, uh, to capture and extract curved edges from the image. So the line segment extraction is to mainly extract key line segments, and then the curved segment extraction aims to extract key curve, um, curved, curved segments. And in the last step, all these extracted segments are consolidated to output saline edge segments, and we need to ensure that, um, the extracted segments represent the most significant and the most visually striking edges in the image. So once we have the edge segment extracted, Um, the next question is how to measure image similarity. So once we have done the, um above process to all the images, including all the images in the database. How do we measure? Um, if two images are similar or not. So, um, we show the process here. Um, basically, we can achieve this by comparing the lines extracted from each image, and these lines often refer to as segments. They just act like, uh, like the fingerprints for the image in this case. So, um, how do we compare the set of lines? Um, So, um, this is basically what happens in these equations. Um, so the complex looking equation might seem very complex at first, but, um, they essentially boil down to this. We compare each segment from the query image against segments in the database images. So, in simpler terms, Um, imagine, um, each image is a collection of lines. So here we basically just represent each image as a collection of lines. And then once we've done that, we just compare these lines one by one. And by comparing, uh, a collection of lines against another collection of lines, we know how visually similar these these two images are. Um so here you can see that we represent the query image with a set of lines, so here on the left. We have the, um, saline edges for the query image. And we represent We represent this image using a set you can see from the brackets here. Uh, so we represent that, uh, with a set of lines, uh, called CQXY and then on the right, we have a database image. We, uh, use CTP to represent that. And then we also represent this database image as a set of lines. And so here we have a set consisting of C TB. So here, the, um, the C in lower case, Um, So the C in lowercase CXY just represents a Coverlet with Centroid at position XY. So it just represents a, uh, segment of edge, uh, at position XY. Um, Now we just look at a specific edge in a query image represented by a location in the image. Um, So, for example, if we look at this little piece of edge here, suppose that it is, um, located at position XY. So what we do is we want to find the best match for this edge within the database images. And this is where the concept of minimum distance comes in. So So the concept of, um, best match basically just means you want to find the edge. Uh, that forms the minimum distance with the quarry edge. So what we do here is we consider a neighbourhood around the same position in the database image. And then within this neighbourhood, uh, so say the original position is XY, and we consider this neighbourhood to be around a region of Delta X and Delta Y. So we just consider that neighbourhood at the same position in the query. I, uh, sorry in the database image. And then we just search for the closest matching edge within this local area. And this is why you have this X plus dot X and plus dot Y here. And so we just have these minimum distance. Um uh, symbol here. Uh, it just means that we want to find the best match, and then we, uh, record the minimum distance value here. So this process just repeated, uh, is repeated for every edge in the query image. So we do the same to all the edges and edge segments in the query image. And then the overall similarity between the two images is then just calculated. Um using this equation. Uh, so the overall distance is just obtained by summing up all the individual minimum distances for all the compared edges. And then this combined distance just reflects how Well, um, the two sets of edges are aligned. So, um, you don't need to, uh, need to know, like the the the all the details in these equations, but just just understanding the big idea would be enough now that we know how to compare two sets of, um uh, edges. Um, there's a, um, one key question here. Um, so how is the distance defined? So we know how. Um, we need to compare, uh, each pair of, um edges and then compute the one with the minimum distance. But how is this distance calculated? So here we, uh, these equations just represent how, um, the similarity, um, of a pair of edges are calculated. And again, you don't need to go through all the details in the equations, but just understanding, um, the big I just need to understand the big idea. So here we compare two edges, we consider, uh, different factors. Uh, so we consider their length their orientations. So here length Just means, uh, if they're both short or long or if they're of different lengths. And the orientation here just means basically how T, how tilted or at which angle they're aligned. Basically, on a large difference in either of these aspects would suggest that these two edges are not very similar. So here in these equations, we use a metric that combine these factors together. Um, so it considers the absolute difference in length. So here, um, on this line, um, uh, we we can see that it considers the absolute difference between, uh the length of CQ and the length of C DB. So here a big difference just represents less similarity. And then on this line, you can also see that it normalises this length difference by the maximum length of the two edges. And this is to ensure that the metric works for edges of varying sizes and additionally, on this line, you can see that, um, the angle formed by the two edges is also considered. So overall, this representation just represents the distance between a pair of edges and and then, uh, basically, the larger the distance, the smaller the similarity. Um, so here we just present a bit more details on the original paper that presents this idea. Um, so this paper was published roughly 25 years ago, and back at that time, researchers relied on only hundreds of manually collected images. Um, for training. So, uh, this number is not really, um, possible. Nowadays, as you know, the number of images required for training and doing experiments has significantly increased and, uh, also a bit more experimental details of this paper. Um, so the paper also highlights the challenges of user variability. So here they build a user interface where people can draw, uh, simple shapes like here to, uh, query images in the database. So here is the UI that they design. And interestingly, the study reviewed, uh, inconsistencies in how users draw the same objects. Uh, for example, you can see that for the same two objects. Um, some users might draw the shape. Uh, the users may draw the shape very differently. Um, even though they're referring to the same objects, Um, So, uh, also for this paper, Um, this is a bit of, um, the quantitative, uh, results. So to assess performance, they evaluated how consistently the system retrieved similar, uh, similar images based on user sketches. So here they use, uh, as written here, 16 sketches and measured how the retrieved images are ranked among all a hun. 100 and 37 images. So here the lower rankings just indicate better performance. Because, uh, the desired case is the image retrieved. Uh, should be, uh the desired case is that the desired image or the ground truth image should appear closer to the beginning or should ideally be the number one, the be ranked the number one. So here we illustrate the results of two subjects. So, um, on the X axis, we have the the X axis represents 16 sketches, and the Y axis represents the ranks. So the lower the better, so we can see, uh, basically, um, So on the left, we have one user, and on the on the right, we have another person. So here the study found very different results among different users. Um, so some users can achieve, uh, better retrieval performance consistently than other users. Now, a problem, um, with this method is that it cannot scale, right? So imagine a case where you need to deal with millions of images. Um, so it would not be realistic, um, to extract the sailing edges for each image and then perform the comparison with the query so it will be not realistic to use the workflow, the workflow that we have just discussed. So a key question here is how to obtain representations for the sketch and then the edges that can be used for large scale image retrieval. So, um, back at that time, researchers were going towards a million images. So to address this problem, um, people back then turned to histograms. Right. So this paper introduce introduces a specific descriptor called the Edge histogram descriptor, Um, similar to colour histograms. Um, so you should be quite familiar with colour. Histogram is already so this edge histogram also captures, um, the distribution. So it focuses on the distribution of gradients within the image. So rather than, uh, describing the distribution of colours, this one, it captures the distribution of gradients within the image. So gradients here just means, um, the direction and intensity of changes in brightness. Um, and this is very crucial in, um detecting edges from images. So, in short, the image as shown here it is divided into smaller regions or patches, and then each patch is then represented based on the gradient information within this specific area. So, as you can see here, this little symbol here just represents basically the distribution of gradients on, uh, towards different orientations or towards different directions. So this approach just provides a more efficient way to encode the edge information within an image. And this method just makes it possible to achieve faster and more scalable image retrieval systems. Now, here, let's take a look into a specific example. Um, so the edge, uh, histogram descriptor is also known as, uh, EHD for short. Um so EHD captures the spatial distribution of edges within an image. So it essentially just creates a fingerprint based on the presence and orientations of edges in different image regions. Right. So, um, here on this slide, we just show how EHD is computed. So the first step, um, is to divide the image into smaller sub images or into smaller patches. So this just creates a collection of small regions for analysis. Uh, common grid size used for EHTEHD are, um, normally four by four or eight by eight But, um, the specific size can be different depending on your specific applications. And then within each of these cells, we compute the gradient orientations and insert them into the corresponding histogram being so here we, as you can see here, the basically the area or the length, uh, pointing to different directions. Just show, uh, basically the intensity of the edges, um, or the intensity of the gradients in that direction. So here we weigh each entry by its squared length, Um, based on the assumption that relatively stronger gradient gradients are more likely to be sketched by the user. So here, uh, again, you don't need to know the details and equations. Uh, but basically, here H, uh, the HIJ, uh, just represent, um, the histogram for cell. Uh, IJ and cell IGCIJ just represent the cell at position IJ. Now, here comes the problem. Um, when we have two images and have computed their edge histogram descriptors. How do we measure the similarity, or how do we measure the distance? Um, so we have provided the equations here at the bottom. Um, so basically, we first use the, uh, lowercase HIJ to represent the histogram of the cell at position IJ. Right. So, um, this HIJ in lower case it contains the edge distribution across different orientations at this specific cell position at IJ. So we first perform a normalisation of HIJ against the total number of edges in this cell across all orientations. So here, K just represent, like, a specific orientation at this, uh, in the histogram. So we divide HIJ by the total number of the total number of edges. And then so we have, um, a normalised histogram and the normalised uses gramme is represented by the capital capital on H ID. So we do the normalisation. So remember that we are comparing two images, so we do the same. We do this normalisation to, um, both images, and we obtain capital HIJ and then capital HIJ Delta. Next for the two images, we just compute the absolute value of the difference at each orientation K. And then we add the, uh, absolute value of the difference altogether. So we add the differences at all orientations together, and this just represents, um, the distribution difference between the two images at position IJ. So here we're adding the difference across orientations. But this difference is still for this position, IJ. And this is what happens in the second equation. And then, uh, finally, we just sum up all the DIJ values, uh, for all cells in the image, and we just consider this one as the distance between the two images. So, um, next we have the concept of, um, M peg seven. So this stands for moving picture experts group, Uh, so this is a, um, standard for describing multimedia content, and it provides a set of tools that, um ah, and descriptors to ah, basically describe audio visual content. So essentially, uh, these MP a seven just helps in indexing and retrieving specific types of content within large multimedia databases. Right. So, uh, in this M PE seven, there's also a, um, edge histogram descriptor. But it is slightly different, As you can see, um, this one is proposed in the year of 2020. And this was, uh, introduced, Uh, almost 10 years after the previous one. Um, so this also involves a multi step procedure. Um, So, uh, first we divide the image, which is same as the previous step. Uh, the previous, uh, PhD. Um, So the input image is first subdivided into, uh, four by four non overlapping blocks. So as exactly as what we show here. So each extracted block is further divided into two by two pixel blocks for capturing local edge orientations. So this is what happens here. So we first divide that into four by four, and then we within each block, we have these two by two pixel blocks and then for each pixel block an edge detection, detection or an algorithm is applied. So within each of this block, we detect all the edges in it. So this is to basically identify areas with significant brightness changes. And this is to highlight the outlines of objects and objects and shapes. And then once we have the edges detected, um, the next step is to basically classify the detected edges, um, based on their orientations. And to do this, we just use, um, this five different types of edge operator masks. So basically, uh, we just classify each of the detected edges into one of these five categories. So we have, um, vertical edge, horizontal edge, diagonal edge in 45 degrees diagonal, diagonal edge in 100 and 35 degree and then we have non orientation type edge. So these five different orientations just work as the beings the beings of the, um histograms. Then one, Once we have all the edges classified into one of these five categories, we just built a histogram. So a histogram is just created for each sub image. Uh, and this histogram just keeps track of basically how many edges belong to each of these five categories. And so you can see here we have, uh, B 12345. So this essentially just captures the distribution of edge directions within this particular sub image. And then the next step is to, um, uh, analyse the dominant edge in each sub image. Right? So, um, the edge category, basically the edge category with the strongest presence could be considered as the dominant edge for this sub image. And if, uh and then what we do with this dominant edge is that the being the being corresponding to the dominant edge orientation would be incremented by one. So this is just to tell this to basically encode the key orientation, um, or to further reinforce the key orientation in each of the sub images um, in this, uh, representation And finally, on all the sub image histograms are then normalised, uh, as shown here. And then they are combined into a single data structure. So this is often just referred to as a feature vector. And you can think of this as, like, the just a feature that represents the overall edge distribution in the image. So in some implementations, we have an additional step, uh, called, uh, global histogram. So sometimes we calculate the global histogram, uh, which considers all edges detected across the entire image. So this So this is what we have in the last line here. So sometimes we also have another entry here, and the global, um, histogram just gives us a broader overview of the overall shape distribution. Uh, sorry. Overall edge distribution. So if a global, um, histogram is obtained, um, we would, um, normally include that as the final element of the future vector and basically the final, um, EHD representation is just shown here. Now, um, let's look at the size of the feature vector. So, basically, if only, um, if only sub image histograms are considered, then here we have, uh, 6. Uh, 16, sub images. And then within each sub image, we have five beings per sub image because we have five different, um, edge orientations. So in total, that would be 16 multiplied by five. So in total, we would have 80 numbers or 80 elements in this vector representation. However, if sometimes we need we, we need to compute the global, uh, global histogram. Then we would have, um you know, one more, um, one more being here. Uh, so we would have, uh, 80 added by five. So we would have 85 numbers in total. So we have five more being five more numbers to represent the global distribution. Um, so here's a little bit of, um, experimental details or experimental results. Um, uh, from this paper, uh, so you can see that with this method, Um, you can query, um, in a 1.5 million image database using, uh, between 0.4 and 3.5 seconds. Um, and so, uh, at the bottom, we have a visual example of the, uh, retrieval results. So you can see, um, basically, the first matters, uh, provide very reasonable answer to the user query. And here we have some more, uh, visualisation results. Uh, so in the top row, we have, uh, So, basically, in the top left corner, we have the user sketch, and then on the top row, uh, we show the expected results, and then, um, at the bottom row, we have some unexpected results. Um, so basically, back at that time, the system still needs significant improvement. Right? So, um, well, this method was fast. It wasn't fast enough, right? So And this had led to researchers to, um, consider how to build an index that could handle a much larger number of images. Um, so here, the key to lies in understanding what makes up an image. Right. So we have images made up by pixels, uh, and similar to how we understand images. We need to understand what defines a sketch as well. So researchers back then just, um, they introduced a concept called, um agile. Um, so, uh, an agile just stands for edge pixels. So the idea here is that, um basically, um, an image can be broken down into a collection of edge pixels or edge segments as written here. Um, so each each edge pixel or each edge or has its own unique properties. And in an image, each pixel has a location X and Y coordinates. And if a pixel happened to sit right on an edge, then this pixel should also have an orientation. So, um, and this is basically how an agile is defined. So an agile just builds on the concept of a pixel by further incorporating, um, direction. So it's essentially just a specific pixel location with a particular direction associated with it. So here, let's kind of go through these representations. So, um, let's use a set of edges to represent the contours of an image. So here we use this, uh, D to represent a set of edges. And then and then we use this P to represent a single Edo. And, uh, this single Edo is actually, um, it belongs to this agile set. And an A is normally defined by, uh, kind of three attributes. It's SX axis, Y axis, and then the gradient orientation. So here we define it, we represent its position as XP. Uh, so it has XP and YP as its positions. And then, uh, we have its gradient orientation represented by theta P So by representing both the sketch and images in the database as collection of S, um, we can compare them effectively. Right. So, um, here to measure the similarity, uh, researchers here often employ the cham for distance. Metric. So here we have the equation for it. Um, so the basic, uh, temple distance from a database image. D. So this D here represent a database image. So the cha distance from this database image, uh, to this query CQ is defined using this equation. So the here, the idea is that for each in the database image P. Um, so here P just represent a single edge in the, um, uh, database. So for each ao, we go through all the ils in the quarry sketch and then find the one that is closest to its position. And we achieve this by this minimum, uh, minimum operation, and then we compute the minimum distance here. So once we have this minimum distance for this single OP, we just do the same to all the edges in the database image. So we have this, um, summation symbol here. So we do the same to all the ailes and then sum all the minimum distance altogether. And then once we have the sum sum, we just average that by the total number of edges in image in image, uh, in the database image. D. Um, So here's the equation. Um, and we'll have a more specific example here. Um, so let's first look at, um, how Athos are represented. So suppose that you want to find a match for this query shape on the left, and then you're matching it against a shape, um, in the database on the right. So the process just involves examining each point on the left, finding its best match on the right, And then for each pair of best match points we found, we calculate the distance. Now, let's take this specific point, um, or this specific agile, um, as an example. So here this edge pixel is represented by a triplet, XY and theta. Um, so for this point say, let's say that it's located at X axis at, uh, at six, across X axis and seven in Y axis. So the first two elements we have 67, and then in terms of the orientations, uh, we can obtain it by, uh, basically, uh, computing the gradient of the edge here and say we have computed that and obtained the orientation to be 45 degrees. Right, So we have 45 degrees at the third element of the triplet. So, um, in agile representations, the orientation space is normally equally quantified into, uh, six beings or six channels. So, uh, this is this is like, uh, performing, like, a disc critic organisation. So we just divide. We can just kind of group all different orientations into one of the six groups. So we have from a negative 15 to 1515 to 45 et cetera. Until we have we reach 100 and 35 to 100 and 65. Um, So if we compute the gradient to be around, uh, 45 degrees, then based on this categorization rule, um, we, uh, this this is considered to be in the orientation. Uh, orientation being number one. So here we have this specific agile represented by this triplet. So we have 67 and one. So, um, if you want to compare the query sketch against this shape in the database, what we do first is, uh, what we do Is that for the first edge in this query for exam, for example, for this first one, for this one, we just go through all the edges in the database image, and then we find the best match. Say, we have gone through all the in this shape, and then we found that this one has the minimum distance with this one with the, uh in the quarry, and then we just compute that distance, and then we just go to the next, uh, edge in the query sketch. So for the next point here again, we go through all the points, all the edges in the database shape. And then we found the one that, uh that is the best match for the second edge in the query, and then we compute that distance. So we just do this to, uh to all the points until basically, we reach the end of the query. Um, and then basically, after we reach the end, we have a collection of minimum distances, and then, uh, we just add all these distances together, and then we average the sum by the number of edges, and this is basically just pretty much what we do. Um uh, in this equation. So, um, based on this calculation, you may found that this distance calculation is actually not symmetrical. So, in other words, the distance, uh, if you calculate the distance from Q to D and then from D to Q are actually not the same. So when we match Q against D So when we match Q against D, we go through the edges in Q and find the best matches in D. And when we match D against Q, we actually go through, Uh, we first go through the edges here in D, and then we find that it's best matching Q so these two processes might generate, you know, they would generate different results. So to address this, uh, researchers here normally just calculate the distance in both directions. So that's why we have, um, these equations here. So, uh, basically, the final distance is computed by taking the average of the distances in both directions. Now, the concept of so, um then then So the previous one was basically, um, Cham for matching. And then the next concept, um, is oriented Cham cham for matching or oriented cham distance um so this metric, uh, was introduced later? Uh, so it only compares the distance between points when their orientations are aligned. So in simpler terms, it only cares about matching points, Uh, with the same angle. Um, basically, if the angles are different, then there's no point in calculating the distance because they can't possibly be a good match. So this is just the key idea of oriented cha matching. Now, um, a major shortcoming of the oriented Cham for matching or OCM. Uh, it's its scalability issue. So with this method, uh, we have to compare a query sketch in with all the images in the database and store their distance. Uh, and, uh, because of this, uh, temper matching based methods are rarely used in large scale applications. Therefore, to build a practical, sketch based image search engine with OCM. Um, so we need to develop a more efficient index structure for it. So, um, so this is like another contribution made in this paper. Uh, So researchers back then just ca me up with the idea of heat map. So, um, the basic idea of, uh, the heat map is basically to transform the complex distance computations that we have just discussed. So we want to transform that into binary into computing binary similarity maps. Right. So here is how it works. So suppose we have a sketch like this one. So suppose this is our sketch, then, uh, we can actually, uh, decompose this sketch into different components based on their orientations. Right. So here we, uh, based on the orientations, we, um, decompose that into three parts. So, uh, for each orientation, we have, uh, certain components of the sketch. And this is actually very similar to how we can decompose a colour into, uh, red, green and blue components. So remember, previously we have, uh, disc criticised, uh, the, uh, orientations into six beings or six orientation groups. So here we also have six channels here. So basically, for each sketch, we can decompose that into different components. We can decompose that into six channels, Uh, and each one just represent the edges in that specific orientations. So in this case, we only have three edges here. So, uh, we only have, uh So here we just have, like, three, channels, uh, with edges on it. So the key question is how can we measure the similarity between sketches using this method? Um, so if we just take a look into this example, So suppose we want to compare the sketch on the right against the contour, Uh, in the database, on on the left. So suppose on the left this one is some, uh, image from the database. So we can also do the decomposition to this contour. Similarly, uh, we can decompose that into, uh, different channels based on the orientations as well. So here in this case, uh, just as a simple example, we have two edges here. One is, um, in orientation, zero degrees, and then one is in orientation, 90 degrees, so we can decompose that into one edge in zero degree group and then one edge in the 90 degree group. Um, now we just consider the query, and then the database, um, overlaying them. We can see that, um that we can see that there are, um, some overlap between the lines at angle at angle zero, and we do not see any overlaps in all other orientation channels. So this overlap here at orientation zero. it signifies a shared element. So this is like a heat between the sketches. And this is also why we, uh, call it, uh, heat maps, because this is where the two edges heat together, heat each other. So remember, we say that for heat maps, we just transform the distance maps into kind of binary, um, similarity calculations. So basically here, uh, we we just have one for angle zero to represent overlap in this orientation, and then we have zeros for all other orientation channels, which means that there's no overlap in those channels. So in this way, we can just represent the overall similarities between the two sketches. So, um, you can see that, um, basically, this approach is very powerful because it significantly simplifies the similarity matching. And, uh, with this method, we don't need to do the complex, uh, pairwise matching for all the S instead, um, we can just rely on this heat map calculation for a much more efficient process. And there's also one more detail. Um, So if you look into the mathematical equation for computing the heat maps and their similarity, you will notice that the similarity between the two directions, uh, is also not symmetrical. So here we also need to consider both directions from Q to D and D to Q when we compute the final similarity. So now if we have a very large database, um, matching a sketch to images can still be very slow because it requires checking every single image detail against the query sketch now inspired by the search engine techniques again, in the the same paper researches, they further implemented an inverted index strategy to further, uh, speed up the retrieval process. Um, so imagine each image detail or each, um, el as a word. So this is pretty similar to text document retrieval as well. Um, so this the system just treats these edge pixels as words. And then the system builds a large dictionary listing every possible agile across all images like this one. So we can see we have the image now and then we have, um, the el, uh, with a certain direction highlighted on it. So here's how it works. So first we built a dictionary. Um, so here we have this dictionary. Um, the systems just considers the sketch resolution and divide the orientations into sections. And then this just creates a dictionary with entries like X and Y and angle. So, uh, in this example, um, normally, we would have, uh uh, 2200 and 40,000 entries. So once we build the dictionary, next is to create the index. So for each entry in the dictionary, the system would create a list of all images containing that specific edile. So here we have two examples. So here for this red edge at this specific position pointing in this specific angle. So for this specific Edo, uh, they found, you know, all these, uh, images that contains these agile, and they've built up them together in the index. And then in this example, we also have this blue one, this blue agile position at this specific location and then pointing in this, uh, in a specific orientation. And here we we can also see that we have a collection of images that contain these agile. And then, uh, basically, the index just have, uh, all these images, uh uh, collected altogether. So this just acts like an inverted list because it flips the focus from images to ils. And then, um, the next step is to match sketches efficiently. So once we have a sketch query. Uh, so normally, we first apply a bit of smoothing. Um, because we don't want the features to be very sensitive to noise, and you'll probably remember what we do. Uh, we do the same when computing the colour histogram vectors. Right? So we also do this moving to kind of make, uh, make it more robust. So once we have the image, we obtain its heat maps and then the system just use the index to quickly find relevant images. All right, so suppose that we have these ils and then, um, the system can just use the index. Uh, so we we found these edges in the dictionary, and then the system just used this index to find, um uh all the relevant all the images relevant to these edges. So just like this, um, so basically, it just checks the heat map for non zero elements and looks up those locations and angles in the index, and then the system finally just ranks the images. So the system just retrieves a list of images containing, uh, containing those, uh, matching edges edges. So we have all these images as the images containing the same edges, and then each of these images would get a heat for these matching elements. So finally, just calculate the score for each image by considering the total number of heats and the total number of agile in the image. So basically here, images with, um, higher scores are considered more similar to the query sketch. So here suppose that we just have these two edges. So from the index, we found a collection of images that matches the first agile and then a collection of images that matches the second one. And then when we do the ranking, we simply count uh, basically the number of hits uh, made by obtained by these images. So if you here take a closer look, you realise that basically, um, this image is the same as this image. Uh, so, um, in simpler term, basically, uh, in this simple image, we have two matching, uh, edges. So this would be ranked, uh, to the top, and then another one is also this image. So this image is also the same as this image, so we can see for this image. We also have, uh, both edges matched. So this is this image is also ranked to the to the top, as shown here. Um, so next we'll have some, uh, bit of experimental details, but, um, let's have a break of 10 minutes, and we'll come back later to discuss the experiments. All right, Now, um, let's come back and continue to discuss some, um, experimental details. Um, so we talked about, uh, this indexing the indexing for, um, ils and heat maps and then for the experiments. Uh uh. Basically, we need to extract the saline edges from the images. So here we, uh normally, we first use a specific type of edge detector, such as the canny detector to obtain all the edges. And then after applying the detector, uh, we philtre the image to identify the boundaries and then by removing the noises, um, we can ensure that the key edges are preserved, and this is to show that ah, you know, when you have an image of size 500 by 500 we need a vocab vocabulary of this size to represent that. So we have 500 by 500 multiplied by six orientations, and this is just to show that if we further scale it down. Uh, we can make it more efficient by using, uh, 240 K codes. Um, so still a bit more experimental details or experimental results. Um, so here here are some statistics about the system. So here we can see that the system runs very well with a reasonable resource footprint and only takes about 2.5 gigabytes of memory. And then, um and then after that, uh, I think they further scaled up the system to a database of 10 million. Um, and then they were able to handle the system, um, within a within an eight gigabyte of memory limit. And this is just to show you, uh, basically the progression made throughout the development process. So, um, so, two years after that, uh, researchers further scale things up to handle billions of images. So, um, this next approach might seem less fancy, but, uh, kind of more engineering focused, But that's often where, um, innovation happens. So this is a paper in AC. M multimedia. Uh, 2013. Um, so imagine that the system works similarly to how search engines handle images. Right. So here, instead of images, um we have edges and then distance maps. So here we convert the images, uh, into edges and distance maps. And then they also leverage the OCM the oriented cham for mapping, uh, matching. Uh, but they introduce a, um, crossover dot product reformation cham for matching, uh, within this process. So this is a method to convert a sketch or image into a vector representation that will be used for further calculation so their actual novelty lies within. Ah, this part. So basically a vector calculation is often, uh, less computationally intensive. So this is why that they can further push the efficiency even further. So here, below the dash line, we have the online process. So this is what happens when you have a, uh, query sketch. Uh, you can compute its query features and then you can compare against the index. And then, uh, after the comparison, you can obtain some retrieval results. Um, so here's an illustration of the, um, the crossover dot product reformulation. Uh, that I've just mentioned. Um, So the problem that they aim to address here is that OC MS matching function requires intensive computation and large memory cost, so it is inefficient to generate the distance maps, Um, online for all the database images. So here, a more compact representation for raw edges and their efficient OCM are highly desired. So here to develop a more compact representation and speed up the matching, um, they first convert a sketch or image, um, into a vector representation. So this is kind of what happens here. And then afterwards the OCM can be conducted by the dot product of these vectors. And, uh uh, for this one, you just need to understand the big idea, and you don't need to go through the details. But basically, there are some mathematical equations that prove this in the paper, and you can take a look into the paper for the, um, uh, math details, Uh, if you're interested. Right? So so far, we have pretty much covered the representative conventional methods for sketch based image retrieval, so we can see that, um, traditionally sketch based image retrieval has relied on handcrafted features like edge maps and distances. And, uh, these methods have been quite successful. But recent years, we all know that, uh, recent years have seen the rise of deep learning, uh, in almost all fields, um, in computer vision. So now, deep learning just allows the system to learn the best feature automatically from the data, Uh, rather than manually designing these features. So the shift from, um, conventional methods to deep learning just opens up, uh, many new possibilities for sketch based retrieval. So here we mainly introduce two areas of research. The first one is category level SBIR. And the second one is fine grained level, fine grained instance level SBIR. Um, so, category level, uh, retrieval aims to find images that belong to the same general category as the sketch. So as we can see here, if we provide a sketch of a helicopter, uh, we want to retrieve, um, uh, retrieve images of other heli helicopters and different to this one for fine grained, uh, approaches. Um, we focus on retrieving highly specific images based on the sketch details. Right. So imagine that we have a large database of, uh, different types of chairs, and then this is the choir. We will provide the system. So, um, so, basically, in this case, we would expect the system to retrieve the chairs with, uh, similar design details like the ones in this sketch. So although they're all, um, uh, belong, they all belong to the same category. Uh, they actually exhibit very, uh, subtle and very fine grain differences. So, uh, this fine grain retrieval would require a more stronger discriminative capability of the, uh, deep learning model. Um, so here, I'd just like to quickly go through, um, some of the fundamentals again, just to provide you with a better context of deep learning models. Um, so here, let's break down deep learning into simpler terms, especially in the context of image processing. So here we provide you with an example of, uh, basically classifying the the handwrit digits, um, into numbers from 0 to 9. So here, uh, you have an image, which is the input, and you want the computer to recognise what it is. Uh, so which is that which is the output of the model and the deep learning? Basically, just achieve this through, uh, multiple layers and a key layer. Here is the convolution layer. So the convolution layer is just about applying a philtre to the image. So we have an animation here to show the convolution process. Um, so this philtre is like a small grid of numbers and it slides pixels. Uh, it slides pixel by pixel across the whole image. And then at each location, it multiplies the corresponding values in the image with its own values, and it would generate a new value. So, uh, this would just create a new value. Uh, and then the created values of all positions here just form, uh, a, uh, new matrix, and we call it the feature map. So, uh, there can be multiple layers here. Sorry. Multiple philtres here. Uh, so we can use multiple, uh, convolutional philtres at the same time. And that would give us, uh, feature maps of multiple aspects. Uh, feature maps of multiple channels and the feature map of each channel. Just capture the characteristics of a specific aspect. So another step is just is called pulling. Um, so pulling reduces the size of, um, the the size of the feature map by taking the maximum value, uh, or the average value within a small region. So here we have Max pulling here, and another type of pulling is called, uh, average pulling. And by extracting the maximum, uh, values out of a feature map, this helps us to focus on the most important regions in the futures. So, um, deep learning networks have many layers of convolutions and pulling, and then they're stacked kind of one after another. And then this just allows the network to learn, uh, very complex features from the data. And then finally, normally we would have, uh, fully connected layers. So this is what we have here. Um, so these a RE layers that take the features and map them to the final prediction or the final predicted probabilities. So the important takeaway here is that after training, um, the Deep Learning Network can extract features from any new image. And, uh so we have the deep features here, and actually, uh, the features at each specific layers would have specific, uh, meanings. Um, so these features are very powerful representations, uh, and they can be used for various tasks, including, uh, sketch based image retrieval. In essence, deep learning just learns the best features for identifying objects or characteristics in images, and, uh, this can be very beneficial. Uh, when we do searching, uh, or retrieval based on sketches. So, uh, when it comes to category level retrieval, uh, researchers often build a classifier using deep learning. So, um, this classifier as an example, take a sketch as an input, and then it predicts the category that it belongs to. For example, it predicts this one into a boat, house, tree or cat, and then the system would just search for, um, images that belong to the same category as the sketch. So, uh, here, this slide just visualises the features learned by the Deep Learning Network. So, um, these features are powerful representations of the sketch and are used for comparison with image categories. So from this visualisation, we can see how how the network, uh, basically learns over multiple layers. So, um, the first few layers capture the basic edges and shapes, so you can see here we have some very, um, straightforward or, um, edge shaped features. And then as you move deeper to lay, uh, as move to the deeper layers like the middle layers or, um, the last layers shown here, um, the network learns more complex patterns and combinations of features, and they allow for more accurate classification of more advanced tasks. So, um, this is an important paper in the field. Um So in this paper in, um, BMVC, uh, 2015. Um, the authors, um, conducted this experiment with this data set, um, with 250 categories and 80 sketches for each category. So this is the largest human sketch data set back at that time. And And the, uh the amazing result is that they proposed a deep neural network model, and, ah also conduct experiments with human beings. And, uh, their model, amazingly, managed to yield, uh, sketch recognition performance that is better than that of humans. So, uh, this was a great success back at that time. So here is an illustration of their model structure. Um, so the success here just mainly lies in their consideration of the specific characteristics of freehand sketches. Um, basically, um, human often make sketches in a sequential manner. So we've shown these sequential manner here. So when we draw an alarm, for example, we would normally, uh, first draw the overall shape of the outer boundary, and then we would then go for the details after that. So, um, to kind of put this one into consideration, they build a multichannel generation, uh, module like this one that encode the sequential ordering in this sketching process. Secondly, due to the free had nature, the same object can be drawn with very different levels of details. For example, A like a human figure sketch can either be a stickman or a portrait. So, um, it really depends on the different drawing styles of the drawer. So to address this specific characteristics, they design a motor scale network to account for such different level of abstractions in the learning of their of their model. So you can think of deep learning like building with Legos. So you have various building blocks that you can combine in different ways to achieve your design output. Um, so in this work, the researchers, they are interested in handling sketches with different levels of detail and complexity. So that's why they might use networks with different branches or different paths to analyse specific aspects of the sketch. So this is a very early work, but you can kind of see how people, uh, try to obtain inspirations from the specific characteristics of data, and they try to incorporate that those characteristics in the design of their model. In the learning of the data that would um, kind of help the model to achieve better performance. Um, and the good news about deep learning is that once you have the model structure designed, the training process can be fairly straightforward. Um, so we have many deep learning frameworks. Um, and they came in. They come with built in training algorithms, uh, for example, with, uh, back propagation. So in terms of the implementations, you don't need to manually calculate everything you simply call the training function. And then, uh, the framework, uh, would just be trained. Um, uh, quite easily. So. That was a very important work back at that time. And, uh, with that Well, we, uh, with with that paper, we've talked about category level, sketch based image retrieval. And now here is another level of retrieval. Um, So, uh, this is the fine grained, sketch based image retrieval. Or sometimes we call it instance. Level, uh, in instance, level image retrieval. So, uh, in this example, we have a database of different shoes, and we want to find the exact one. that matches the sketch, right? So we have many different types of shoes and we have a sketch like this one and we want to find the best match. So, um, we can see that in order to achieve this, the model needs to be able to discriminate or differentiate the fine growing differences between these shoes of different styles. So here this work addresses this task by, um, formulating a cross model task. And so that's cross model. Um, so because we're searching for colour images based on a grey scale sketch. So this is called a cross modal retrieval, as we are using data of one modality, which is the sketch to search for data of another modality, which are the colour images. So we consider the sketch and then the colour images as separate modalities. And then, uh, once we have that idea, uh, we know that, actually, um, deep learning comes with a technique called multimodal learning. Uh, so the goal is to bring two different types of data into one common space. So we can kind of use that tech technique to, um these context to address this sketch retrieval problem. Right? So you can think of that, um, as translating between languages. So for example, uh, English and then Chinese, they might express the same meaning with different words. So similarly a sketch and its corresponding coloured image. Um, they also correspond to the same object. But they are represented in different modalities. They're represented in different ways. Um, so the goal here is to bring the sketches and then bring the sketches and the images together into a common space. Once they're in the common space, it will be much easier for us to do the searching. So here the system that uses an encoder to So we have the sketch and then we have the picture. Um, so the system uses an encoder to process the sketch, and then it uses another encoder to process the image right. And ideally, uh, we can see that this sketch is a good match. Uh, so this this sketch and this image, they form a really good match. So ideally, in this case, the features extracted here, they should be very close. Um, they should be very close in the future in the future space. So this is kind of the key idea behind this work. Now we understand how to bring sketches and images into a common space. So we do that through an encoder But how do we train the system to recognise similar pairs across these modalities? So here's the challenge. So we want the features of a sketch and its matching image. So the the the sketch is this one, and its matching image is this one P Plus. So we want the feature of them to be close in this shared space, right? But at the same time, we also need the system to recognise that sketches and images of different objects should have featured further apart in this space. So we want, um, the sketch and then its matching image to be close up in the feature space. And at the same time, we want the sketch with other objects or other images to be further apart in the feature space. Right? So normally, um, in train in deep learning in deep model training, we use, uh, a loss to minimise the arrow between the predicted results and the ground truth. But in this case, the loss is slightly different. So here we use a special training approach called the triplet loss. Um, to kind of achieve this objective. Um, so we use, uh So here we for training. We need, uh, three inputs or three images. So we need a sketch or as the query, and we need an image of the same object as the query. So they form a positive pair, and they will also need an image of a different object. So this one would form a negative pair with the query. So the goal of the, uh, triple loss written here is basically just to push the positive pairs features closer. And then, uh, simultaneously pushing the negative pairs features further away. Right in this time, in this way, the system learns to basically distinguish similar and dissimilar pairs across the sketch and then the image modalities. So this is kind of how it works. And to achieve this, we actually need, uh, we need to have sufficient paired data to train the model. But if we have sufficient sufficient data, um, this model can be trained, and then it it can, uh, recognise similar and dissimilar pairs. Um, so here we have a bit of, um, experimental details. So, um, here we just show more details. Um, the experiment data sets so the authors use and, uh, used on an instance level S, PR data set. Um, so, uh, it contains, uh, 716 pairs of shoes and chairs. Uh, and then these sketches are drawn by amateur people on touchscreen devices. And then, in order to do the training, we also need to manually, um, label some, uh, triplet, uh, triplet pairs or triplets. Um, so these annotations just tell us which of two photos is more similar to a given, uh, to a given sketch. So this will help us to, uh, train the laws and help the system to learn how to rank images accurately. Now, these slides compares the proposed method with existing retrieval approaches. Um, so the compared existing methods, uh, here include, um the back of words method? Uh, HHOG, uh, which is histogram of oriented gradients. And then we have support vector machine SBM. Uh, so here the true matches are highlighted in green, and then the method, the results obtained by this method is at the very bottom. So we can see from the results that the proposed method generally retrieves the most relevant image as the first rank. And here we also have some quantitative results to show the effectiveness of the model compared to existing ones, right? So, um, once we have a basic model, the next step is to achieve style agnostic SBIR. Right. So here it's it's motivation. So the motivation is pretty clear, and it's pretty straightforward, right? Basically, not everyone is good at drawing, and, uh, different users can have very different skills or very different drawing styles. Right. So, for example, if I give you this six sketches here and if I ask you how many photos they represent, well, you may say that they represent six photos, but actually, uh, they represent, uh, two images. Right? So these three sketches, um, at the top, uh, they represent this black shoe and, uh, the the three sketches at the bottom. They represent these pink orange shoes. So this is just to show you that basically, um, different users sketch the same object instance very differently. So the goal of this work this paper from CV PR 2021. Uh, the goal of this work, basically, was to find the intended photo, despite the variations in drawing styles. So here, on the right, we have an intuitive comparison between an existing method shown in blue and then the proposed one shown in yellow. So here, uh, for the existing model, without considering the style, diversity and existing SP I model, they can, uh, yield completely different results for different sketches. But in fact, these three sketches, they are sketched by different people for the same object. So, uh, they're actually, they actually should be, Um they're actually for the same object, right? Uh, and in this method with the star agnostic model, the same intended object is retrieved. So, um, current methods normally struggle because they treat all details in a sketch as equally important for searching. Right. So they treat the whole image as, uh, the different parts of a sketch as equally important. And this can lead to inaccurate results when sketches have different styles. So the key idea of this work is it's fairly simple. Uh, so it's to separate style from content and only use the content features for retrieval. Right? So the core idea here is a disentanglement model, which is the part in the middle. So for the disentanglement, they use a cross model Translation variational auto encoder, VA E, which is a widely used model for disentanglement. So basically, they use a VA E model. Uh, it takes a sketch or a photo image as input, and it it decomposes its content into two parts. Right. So one part is the, uh, model, uh, model, specific part, and then the other part is the model invariant part. So this model specific part in the case of sketch just corresponds to the use of drawing style because it contains the feature that is specific to this modality, right? And then this model invariant, uh, part just corresponds to the content of the sketch or the content of the image, because basically, it is invariant of the modality of the model. So it is just the content, the actual content or the actual objects represented by, um, the drawing of the image. So once we decompose the feature into two parts, the, uh, retrieval triplet triplet loss is just applied on the model invariant features. So we only use the content, uh, to train the retrieval part. So in this way, we just consider the content part only for retrieval. And we we eliminate the style, the style part so that, um, we minimise the influence of the different drawing styles. Uh, and they further want the disentanglement to be able to dynamically adapt to new user styles. So they further propose a meta learning strategy. And we can see this kind of from this inner loop and then this outer loop with this meta learning method, uh, which means, uh, to train the model, learning to learn. Um, but we're not going to go through, uh, this part in detail and you can read the paper if you're interested. So, um, here's just a little bit of some more experimental details for this work. Uh, so these are the database the data sets that they use for experiments. And then here we show some of the, uh, quantitative results to show the effectiveness of the work. So here you notice that we have some, uh, evaluation metrics, so we have accuracy at one and accuracy at 10. So accuracy at one here just measures the accuracy when the most relevant image is actually the correct match for the query. So this one focuses on finding the single best result. So the single top one result and then this accurate accuracy at, uh, accuracy at 10. Um, this is a more relaxed measurement. So it looks, uh, it looks at the top 10 retrieved images and checks if the relevant image is included in the top 10 results, right, So it just reflects how likely you're able to find the right image within the 1st 10 results. So this one, this accuracy at 10 metric just provides a broader image of how successfully, uh, the images are retrieved, uh, or how successful? Uh, the retrieval system is in finding relevant images, and here we have a bit more visual results. So, um, here the irrelevant images are marked with a cross sign, and then we can see, uh, that the proposed method here at the bottom significantly reduces the number of irrelevant images retrieved compared to, um, the baseline model, Um, at the at the at the top part. And then here is the comparison of the retrieval ranking. So here it shows the rank of the ground truth photo in the retrieved set of images. So here the ground truth photo is ranked three, and here is eight. And here is four. Then for the proposed one, we have all these images ranked number one in the results. So we have 111. so 111 is smaller than 384. Meaning that, um, you know, the the proposed method tends to rank the most relevant images higher in the list. So this means that with this proposed method, you're more likely to find the best match sooner in the search results. Um, and then here we have another set of comparisons again for the baseline, we have the ranks to be, uh, 735. And then again, for the proposed one, we have 111, so we can see we have some, uh, same conclusion with the previous one. And, uh, here is a summary of what you need to know for this, uh, lecture. So today we talk about, uh, especially we talk about the sketch features, uh, the S PR in these applications, we talked about the deep learning based image retrieval and then deep learning based methods for SPIR. And here's just some some references. Uh, so we'll just stop the lecture now for today, and, um, I'll see you, um, next week or later in the tutorial.

SPEAKER 1
I have some questions about final example. So, I, I feel that you know so many. OK, for the family gonna be more focused on mass and co vision or just give a scenario you need to And she lecture in week 12.

SPEAKER 0
So there's a, uh so you he will And then he will provide you with more details for me.

SPEAKER 1
Thank you.

SPEAKER 0
He'll be here this time. Yeah.

SPEAKER 1
You question. Switch fed too early today to pass the whole support. OK, the actual, uh top do my test for it. I think that I think should be ok. Ok, OK. Radio globalisation. I see the situation that mhm OK for it because

SPEAKER 0
OK, just OK.

SPEAKER 1
Just give me Don't give me, you know, I OK password. Can you show what that is? And I will change the topic. And she can. She she told her she should
